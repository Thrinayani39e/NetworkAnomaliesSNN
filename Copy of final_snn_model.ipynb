{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgzHB7u+4mCfIANANe07wp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#mount drive to save models\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pRN9asgI5ndX","executionInfo":{"status":"ok","timestamp":1715055606066,"user_tz":-330,"elapsed":7488,"user":{"displayName":"thrinayani yedhoti","userId":"13975255438692979374"}},"outputId":"4bfd1324-0d38-4586-9af9-8665e5c3b8fc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#import libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","%matplotlib inline\n","from keras.layers import Dense, Dropout, BatchNormalization, Activation\n","from keras.optimizers import Adam\n","from keras.optimizers import Adagrad\n","from keras.callbacks import EarlyStopping\n","from keras.regularizers import l2\n","from keras.models import Sequential\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer"],"metadata":{"id":"cC-rrxXt5gkC","executionInfo":{"status":"ok","timestamp":1715055669805,"user_tz":-330,"elapsed":602,"user":{"displayName":"thrinayani yedhoti","userId":"13975255438692979374"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKv_72WQ5b4z","outputId":"6254fbe8-cde7-4678-ed03-77d23cb46ed3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Train Loss: 0.7338631749153137, Test Loss: 0.6421011090278625\n","Epoch [2/100], Train Loss: 0.6557450294494629, Test Loss: 0.582107424736023\n","Epoch [3/100], Train Loss: 0.5954012870788574, Test Loss: 0.531035304069519\n","Epoch [4/100], Train Loss: 0.5445520877838135, Test Loss: 0.4870692789554596\n","Epoch [5/100], Train Loss: 0.503165602684021, Test Loss: 0.447680801153183\n","Epoch [6/100], Train Loss: 0.4631696343421936, Test Loss: 0.41122519969940186\n","Epoch [7/100], Train Loss: 0.4288637340068817, Test Loss: 0.3772095739841461\n","Epoch [8/100], Train Loss: 0.3960428833961487, Test Loss: 0.3463188409805298\n","Epoch [9/100], Train Loss: 0.36623185873031616, Test Loss: 0.3192221224308014\n","Epoch [10/100], Train Loss: 0.3385574519634247, Test Loss: 0.29532116651535034\n","Epoch [11/100], Train Loss: 0.31656619906425476, Test Loss: 0.2740699350833893\n","Epoch [12/100], Train Loss: 0.2972395122051239, Test Loss: 0.25501129031181335\n","Epoch [13/100], Train Loss: 0.2764638662338257, Test Loss: 0.23834733664989471\n","Epoch [14/100], Train Loss: 0.260419636964798, Test Loss: 0.22362643480300903\n","Epoch [15/100], Train Loss: 0.24370679259300232, Test Loss: 0.21109874546527863\n","Epoch [16/100], Train Loss: 0.23240019381046295, Test Loss: 0.20054981112480164\n","Epoch [17/100], Train Loss: 0.22199362516403198, Test Loss: 0.19209985435009003\n","Epoch [18/100], Train Loss: 0.2117495983839035, Test Loss: 0.18287816643714905\n","Epoch [19/100], Train Loss: 0.20356900990009308, Test Loss: 0.17514154314994812\n","Epoch [20/100], Train Loss: 0.1956801861524582, Test Loss: 0.16814830899238586\n","Epoch [21/100], Train Loss: 0.18622925877571106, Test Loss: 0.16353702545166016\n","Epoch [22/100], Train Loss: 0.18166670203208923, Test Loss: 0.15875087678432465\n","Epoch [23/100], Train Loss: 0.17627395689487457, Test Loss: 0.15352928638458252\n","Epoch [24/100], Train Loss: 0.17020058631896973, Test Loss: 0.1498449444770813\n","Epoch [25/100], Train Loss: 0.16487734019756317, Test Loss: 0.1447959840297699\n","Epoch [26/100], Train Loss: 0.15958715975284576, Test Loss: 0.1407030075788498\n","Epoch [27/100], Train Loss: 0.15514035522937775, Test Loss: 0.13692745566368103\n","Epoch [28/100], Train Loss: 0.14928607642650604, Test Loss: 0.13370592892169952\n","Epoch [29/100], Train Loss: 0.14523710310459137, Test Loss: 0.13220515847206116\n","Epoch [30/100], Train Loss: 0.14375360310077667, Test Loss: 0.12933139503002167\n","Epoch [31/100], Train Loss: 0.13870137929916382, Test Loss: 0.12744638323783875\n","Epoch [32/100], Train Loss: 0.13696692883968353, Test Loss: 0.12486401200294495\n","Epoch [33/100], Train Loss: 0.13247284293174744, Test Loss: 0.12232440710067749\n","Epoch [34/100], Train Loss: 0.12755362689495087, Test Loss: 0.11984223127365112\n","Epoch [35/100], Train Loss: 0.12562884390354156, Test Loss: 0.1176043450832367\n","Epoch [36/100], Train Loss: 0.12307329475879669, Test Loss: 0.11482921987771988\n","Epoch [37/100], Train Loss: 0.11941473931074142, Test Loss: 0.11430909484624863\n","Epoch [38/100], Train Loss: 0.11818333715200424, Test Loss: 0.11242059618234634\n","Epoch [39/100], Train Loss: 0.11519348621368408, Test Loss: 0.11074959486722946\n","Epoch [40/100], Train Loss: 0.11323639005422592, Test Loss: 0.10917508602142334\n","Epoch [41/100], Train Loss: 0.11058037728071213, Test Loss: 0.10752938687801361\n","Epoch [42/100], Train Loss: 0.1106475368142128, Test Loss: 0.10727424919605255\n","Epoch [43/100], Train Loss: 0.10822075605392456, Test Loss: 0.10454783588647842\n","Epoch [44/100], Train Loss: 0.1059674471616745, Test Loss: 0.10304208844900131\n","Epoch [45/100], Train Loss: 0.10357416421175003, Test Loss: 0.10142887383699417\n","Epoch [46/100], Train Loss: 0.10276837646961212, Test Loss: 0.09999783337116241\n","Epoch [47/100], Train Loss: 0.10065074265003204, Test Loss: 0.0986640676856041\n","Epoch [48/100], Train Loss: 0.09724805504083633, Test Loss: 0.09702219069004059\n","Epoch [49/100], Train Loss: 0.09698996692895889, Test Loss: 0.0957501232624054\n","Epoch [50/100], Train Loss: 0.09547561407089233, Test Loss: 0.09398719668388367\n","Epoch [51/100], Train Loss: 0.09206819534301758, Test Loss: 0.09265617281198502\n","Epoch [52/100], Train Loss: 0.09193596243858337, Test Loss: 0.09148920327425003\n","Epoch [53/100], Train Loss: 0.09053895622491837, Test Loss: 0.09168904274702072\n","Epoch [54/100], Train Loss: 0.09021007269620895, Test Loss: 0.09151087701320648\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Load data\n","file_path1 = '/content/drive/MyDrive/Final year project/datafinal/processed_train.csv'\n","data = pd.read_csv(file_path1)\n","\n","# Split into train and test sets\n","X = data.drop(labels='class', axis=1)\n","y = data['class']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=4)\n","\n","# Convert data to PyTorch tensors\n","train_data = torch.tensor(X_train.values, dtype=torch.float32)\n","labels = torch.tensor(y_train.values, dtype=torch.long)\n","test_data = torch.tensor(X_test.values, dtype=torch.float32)\n","test_labels = torch.tensor(y_test.values, dtype=torch.long)\n","\n","# Define SNN model with added hidden layer and dropout\n","class SNN(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.2):\n","        super(SNN, self).__init__()\n","        self.input_layer = nn.Linear(input_size, hidden_size1)\n","        self.spike = nn.Hardtanh(0, 1)\n","        self.hidden_layer1 = nn.Linear(hidden_size1, hidden_size2)\n","        self.hidden_layer2 = nn.Linear(hidden_size2, hidden_size2)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.synaptic_weights = nn.Parameter(torch.rand(hidden_size2, output_size))\n","        self.threshold = nn.Parameter(torch.rand(1))\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        x = self.spike(x - self.threshold)\n","        x = F.relu(self.hidden_layer1(x))\n","        x = F.relu(self.hidden_layer2(x))\n","        x = self.dropout(x)\n","        x = torch.mm(x, self.synaptic_weights)\n","        return x\n","\n","# Training loop with test loss and early stopping\n","def train_snn(model, train_data, labels, test_data, test_labels, num_epochs, learning_rate, patience=5):\n","    criterion = nn.CrossEntropyLoss()  # Define the loss function\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Add L2 regularization\n","\n","    history = {'train_loss': [], 'test_loss': []}\n","    best_test_loss = float('inf')\n","    no_improvement = 0\n","\n","    for epoch in range(num_epochs):\n","        # Training\n","        model.train()\n","        outputs = model(train_data)\n","        loss_train = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss_train.backward()\n","        optimizer.step()\n","\n","        # Testing\n","        model.eval()\n","        with torch.no_grad():\n","            outputs_test = model(test_data)\n","            loss_test = criterion(outputs_test, test_labels)\n","\n","        # Store training and test loss for visualization\n","        history['train_loss'].append(loss_train.item())\n","        history['test_loss'].append(loss_test.item())\n","\n","        # Early stopping\n","        if loss_test < best_test_loss:\n","            best_test_loss = loss_test\n","            no_improvement = 0\n","        else:\n","            no_improvement += 1\n","            if no_improvement >= patience:\n","                print(f'Early stopping at epoch {epoch + 1}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss_train.item()}, Test Loss: {loss_test.item()}')\n","\n","    return history\n","\n","# Initialize SNN model and train\n","input_size = X_train.shape[1]\n","hidden_size1 = 128\n","hidden_size2 = 64\n","output_size = 2\n","dropout_rate = 0.3\n","num_epochs = 100\n","learning_rate = 0.001\n","patience = 10\n","\n","snn_model = SNN(input_size, hidden_size1, hidden_size2, output_size, dropout_rate)\n","import time\n","s = time.time()\n","history = train_snn(snn_model, train_data, labels, test_data, test_labels, num_epochs, learning_rate, patience)\n","t = time.time() - s\n","print(\"training time:\", t)\n","\n","# Plot the training and test loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(history['train_loss'], label='Training Loss')\n","plt.plot(history['test_loss'], label='Test Loss')\n","plt.title('Training and Test Loss over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# SNN on the test set\n","with torch.no_grad():\n","    test_outputs = snn_model(test_data)\n","    _, predicted = torch.max(test_outputs, 1)\n","\n","# Calculate confusion matrix\n","conf_mat = confusion_matrix(test_labels, predicted)\n","\n","# Plot Confusion Matrix Heatmap\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n","plt.title('Confusion Matrix Heatmap')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.show()\n","\n","# Calculate specificity and FAR from confusion matrix\n","TN = conf_mat[0, 0]\n","FP = conf_mat[0, 1]\n","FN = conf_mat[1, 0]\n","TP = conf_mat[1, 1]\n","\n","specificity = TN / (TN + FP)\n","far = FP / (FP + TN)\n","\n","# Calculate additional performance metrics\n","precision = precision_score(test_labels, predicted)\n","recall = recall_score(test_labels, predicted)\n","f1 = f1_score(test_labels, predicted)\n","accuracy = accuracy_score(test_labels, predicted)\n","\n","# ROC curve\n","fpr, tpr, thresholds = roc_curve(test_labels, test_outputs[:, 1])\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","# Print performance metrics\n","print(\"\\nPerformance Metrics:\")\n","print(\"Specificity: {:.4f}\".format(specificity))\n","print(\"FAR: {:.2f}\".format(far))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))\n","print(\"F1 Score: {:.4f}\".format(f1))\n","print(\"Accuracy: {:.4f}\".format(accuracy))"]},{"cell_type":"code","source":["snn_model.save(\"/content/drive/My Drive/Final year project/models/final\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"VbfQOZbVPU7X","executionInfo":{"status":"error","timestamp":1715055017823,"user_tz":-330,"elapsed":589,"user":{"displayName":"thrinayani yedhoti","userId":"13975255438692979374"}},"outputId":"4c11aa2f-42cd-4613-a748-2d8ed705b575"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'SNN' object has no attribute 'save'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-3a8c1c2e9b16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Final year project/models/final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SNN' object has no attribute 'save'"]}]},{"cell_type":"code","source":["torch.save(snn_model.state_dict(), \"/content/drive/My Drive/Final year project/models/final_model\")"],"metadata":{"id":"da4PYeOA5t_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","\n","# Load data\n","file_path1 = '/content/drive/MyDrive/Final year project/datafinal/processed_train.csv'\n","data = pd.read_csv(file_path1)\n","\n","# Split into train and test sets\n","X = data.drop(labels='class', axis=1)\n","y = data['class']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=4)\n","\n","# Convert data to PyTorch tensors\n","train_data = torch.tensor(X_train.values, dtype=torch.float32)\n","labels = torch.tensor(y_train.values, dtype=torch.long)\n","test_data = torch.tensor(X_test.values, dtype=torch.float32)\n","test_labels = torch.tensor(y_test.values, dtype=torch.long)\n","\n","# Define SNN model with added hidden layer and dropout\n","class SNN(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.2):\n","        super(SNN, self).__init__()\n","        self.input_layer = nn.Linear(input_size, hidden_size1)\n","        self.spike = nn.Hardtanh(0, 1)\n","        self.hidden_layer1 = nn.Linear(hidden_size1, hidden_size2)\n","        self.hidden_layer2 = nn.Linear(hidden_size2, hidden_size2)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.synaptic_weights = nn.Parameter(torch.rand(hidden_size2, output_size))\n","        self.threshold = nn.Parameter(torch.rand(1))\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        x = self.spike(x - self.threshold)\n","        x = F.relu(self.hidden_layer1(x))\n","        x = F.relu(self.hidden_layer2(x))\n","        x = self.dropout(x)\n","        x = torch.mm(x, self.synaptic_weights)\n","        return x\n","\n","# Training loop with test loss and early stopping\n","def train_snn(model, train_data, labels, num_epochs, learning_rate, patience=5):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Add L2 regularization\n","\n","    history = {'train_loss': [], 'test_loss': []}\n","    best_test_loss = float('inf')\n","    no_improvement = 0\n","\n","    for epoch in range(num_epochs):\n","        # Training\n","        model.train()\n","        outputs = model(train_data)\n","        loss_train = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss_train.backward()\n","        optimizer.step()\n","\n","        # Testing\n","        model.eval()\n","        with torch.no_grad():\n","            outputs_test = model(test_data)\n","            loss_test = criterion(outputs_test, test_labels)\n","\n","        # Store training and test loss for visualization\n","        history['train_loss'].append(loss_train.item())\n","        history['test_loss'].append(loss_test.item())\n","\n","        # Early stopping\n","        if loss_test < best_test_loss:\n","            best_test_loss = loss_test\n","            no_improvement = 0\n","        else:\n","            no_improvement += 1\n","            if no_improvement >= patience:\n","                print(f'Early stopping at epoch {epoch + 1}')\n","                break\n","\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss_train.item()}, Test Loss: {loss_test.item()}')\n","\n","    return history\n","\n","# Cross-validation with FAR calculation\n","def cross_validate_snn(X, y, num_folds, input_size, hidden_size1, hidden_size2, output_size, dropout_rate, num_epochs, learning_rate, patience):\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    roc_auc_scores = []\n","    precision_scores = []\n","    recall_scores = []\n","    f1_scores = []\n","    accuracy_scores = []\n","    far_scores = []\n","\n","    for train_index, test_index in kf.split(X):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        # Convert data to PyTorch tensors\n","        train_data = torch.tensor(X_train.values, dtype=torch.float32)\n","        labels = torch.tensor(y_train.values, dtype=torch.long)\n","        test_data = torch.tensor(X_test.values, dtype=torch.float32)\n","        test_labels = torch.tensor(y_test.values, dtype=torch.long)\n","\n","        snn_model = SNN(input_size, hidden_size1, hidden_size2, output_size, dropout_rate)\n","        history = train_snn(snn_model, train_data, labels, num_epochs, learning_rate, patience)\n","\n","        with torch.no_grad():\n","            test_outputs = snn_model(test_data)\n","            _, predicted = torch.max(test_outputs, 1)\n","\n","        # Calculate performance metrics\n","        fpr, tpr, thresholds = roc_curve(test_labels, test_outputs[:, 1])\n","        roc_auc = auc(fpr, tpr)\n","        roc_auc_scores.append(roc_auc)\n","\n","        precision = precision_score(test_labels, predicted)\n","        recall = recall_score(test_labels, predicted)\n","        f1 = f1_score(test_labels, predicted)\n","        accuracy = accuracy_score(test_labels, predicted)\n","\n","        precision_scores.append(precision)\n","        recall_scores.append(recall)\n","        f1_scores.append(f1)\n","        accuracy_scores.append(accuracy)\n","\n","        # Calculate False Alarm Rate (FAR)\n","        TN, FP, FN, TP = confusion_matrix(test_labels, predicted).ravel()\n","        far = FP / (FP + TN)\n","        far_scores.append(far)\n","\n","    # Calculate mean and standard deviation of performance metrics\n","    roc_auc_mean = np.mean(roc_auc_scores)\n","    roc_auc_std = np.std(roc_auc_scores)\n","    precision_mean = np.mean(precision_scores)\n","    recall_mean = np.mean(recall_scores)\n","    f1_mean = np.mean(f1_scores)\n","    accuracy_mean = np.mean(accuracy_scores)\n","    far_mean = np.mean(far_scores)\n","    far_std = np.std(far_scores)\n","\n","    print(\"\\nPerformance Metrics:\")\n","    print(f\"ROC AUC: {roc_auc_mean:.4f} ± {roc_auc_std:.4f}\")\n","    print(f\"Precision: {precision_mean:.4f}\")\n","    print(f\"Recall: {recall_mean:.4f}\")\n","    print(f\"F1 Score: {f1_mean:.4f}\")\n","    print(f\"Accuracy: {accuracy_mean:.4f}\")\n","    print(f\"False Alarm Rate (FAR): {far_mean:.4f} ± {far_std:.4f}\")\n","\n","# Initialize SNN model and cross-validate\n","input_size = X_train.shape[1]\n","hidden_size1 = 128\n","hidden_size2 = 64\n","output_size = 2\n","dropout_rate = 0.3\n","num_epochs = 100\n","learning_rate = 0.001\n","patience = 10\n","num_folds = 5\n","\n","cross_validate_snn(X, y, num_folds, input_size, hidden_size1, hidden_size2, output_size, dropout_rate, num_epochs, learning_rate, patience)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kn1rGCfh8rSp","executionInfo":{"status":"ok","timestamp":1714815352374,"user_tz":-330,"elapsed":53990,"user":{"displayName":"thrinayani yedhoti","userId":"13975255438692979374"}},"outputId":"eae75764-7e1d-46ca-9ffd-27006d8bf6af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Train Loss: 0.7520098090171814, Test Loss: 0.6582658886909485\n","Epoch [2/100], Train Loss: 0.6719823479652405, Test Loss: 0.5941689014434814\n","Epoch [3/100], Train Loss: 0.6078024506568909, Test Loss: 0.5433616638183594\n","Epoch [4/100], Train Loss: 0.5578403472900391, Test Loss: 0.5000085830688477\n","Epoch [5/100], Train Loss: 0.5168921947479248, Test Loss: 0.4600018858909607\n","Epoch [6/100], Train Loss: 0.4769808351993561, Test Loss: 0.42323195934295654\n","Epoch [7/100], Train Loss: 0.4419533908367157, Test Loss: 0.38957563042640686\n","Epoch [8/100], Train Loss: 0.40875253081321716, Test Loss: 0.35831359028816223\n","Epoch [9/100], Train Loss: 0.37963181734085083, Test Loss: 0.3298010230064392\n","Epoch [10/100], Train Loss: 0.35299256443977356, Test Loss: 0.3039620518684387\n","Epoch [11/100], Train Loss: 0.3291572034358978, Test Loss: 0.28078967332839966\n","Epoch [12/100], Train Loss: 0.30547448992729187, Test Loss: 0.2602701783180237\n","Epoch [13/100], Train Loss: 0.28672829270362854, Test Loss: 0.24222110211849213\n","Epoch [14/100], Train Loss: 0.26855430006980896, Test Loss: 0.2264901101589203\n","Epoch [15/100], Train Loss: 0.2506119906902313, Test Loss: 0.21439054608345032\n","Epoch [16/100], Train Loss: 0.23813481628894806, Test Loss: 0.20221127569675446\n","Epoch [17/100], Train Loss: 0.22673723101615906, Test Loss: 0.19159230589866638\n","Epoch [18/100], Train Loss: 0.21600084006786346, Test Loss: 0.18204768002033234\n","Epoch [19/100], Train Loss: 0.20654921233654022, Test Loss: 0.17389744520187378\n","Epoch [20/100], Train Loss: 0.19490543007850647, Test Loss: 0.16636332869529724\n","Epoch [21/100], Train Loss: 0.18803824484348297, Test Loss: 0.1593712568283081\n","Epoch [22/100], Train Loss: 0.18060919642448425, Test Loss: 0.15294985473155975\n","Epoch [23/100], Train Loss: 0.17292563617229462, Test Loss: 0.14704777300357819\n","Epoch [24/100], Train Loss: 0.16656826436519623, Test Loss: 0.14197050034999847\n","Epoch [25/100], Train Loss: 0.15846256911754608, Test Loss: 0.13955259323120117\n","Epoch [26/100], Train Loss: 0.1570826917886734, Test Loss: 0.13569307327270508\n","Epoch [27/100], Train Loss: 0.15121519565582275, Test Loss: 0.1311025619506836\n","Epoch [28/100], Train Loss: 0.14653506875038147, Test Loss: 0.12985928356647491\n","Epoch [29/100], Train Loss: 0.1459355652332306, Test Loss: 0.12444590777158737\n","Epoch [30/100], Train Loss: 0.13857515156269073, Test Loss: 0.12211320549249649\n","Epoch [31/100], Train Loss: 0.13491755723953247, Test Loss: 0.11951148509979248\n","Epoch [32/100], Train Loss: 0.13284771144390106, Test Loss: 0.11590924113988876\n","Epoch [33/100], Train Loss: 0.13056638836860657, Test Loss: 0.11365770548582077\n","Epoch [34/100], Train Loss: 0.12650693953037262, Test Loss: 0.11159326881170273\n","Epoch [35/100], Train Loss: 0.12172578275203705, Test Loss: 0.11225973814725876\n","Epoch [36/100], Train Loss: 0.12375316768884659, Test Loss: 0.11032392084598541\n","Epoch [37/100], Train Loss: 0.12078668922185898, Test Loss: 0.10701305419206619\n","Epoch [38/100], Train Loss: 0.11666761338710785, Test Loss: 0.10437358170747757\n","Epoch [39/100], Train Loss: 0.1126733124256134, Test Loss: 0.10174687951803207\n","Epoch [40/100], Train Loss: 0.11050507426261902, Test Loss: 0.09827029705047607\n","Epoch [41/100], Train Loss: 0.10586380213499069, Test Loss: 0.09667763113975525\n","Epoch [42/100], Train Loss: 0.104075588285923, Test Loss: 0.09371217340230942\n","Epoch [43/100], Train Loss: 0.10046731680631638, Test Loss: 0.09165854007005692\n","Epoch [44/100], Train Loss: 0.09765669703483582, Test Loss: 0.08933105319738388\n","Epoch [45/100], Train Loss: 0.09564018249511719, Test Loss: 0.08967506140470505\n","Epoch [46/100], Train Loss: 0.09592591971158981, Test Loss: 0.08690223097801208\n","Epoch [47/100], Train Loss: 0.09277325868606567, Test Loss: 0.08452539891004562\n","Epoch [48/100], Train Loss: 0.08902902156114578, Test Loss: 0.08806397020816803\n","Epoch [49/100], Train Loss: 0.09599200636148453, Test Loss: 0.08568309247493744\n","Epoch [50/100], Train Loss: 0.09149643033742905, Test Loss: 0.0830923467874527\n","Epoch [51/100], Train Loss: 0.08923989534378052, Test Loss: 0.08020874112844467\n","Epoch [52/100], Train Loss: 0.08391696959733963, Test Loss: 0.07809749990701675\n","Epoch [53/100], Train Loss: 0.08258315920829773, Test Loss: 0.07598230987787247\n","Epoch [54/100], Train Loss: 0.0790572240948677, Test Loss: 0.07766097784042358\n","Epoch [55/100], Train Loss: 0.08055609464645386, Test Loss: 0.07588257640600204\n","Epoch [56/100], Train Loss: 0.07940753549337387, Test Loss: 0.07510581612586975\n","Epoch [57/100], Train Loss: 0.07819288969039917, Test Loss: 0.07548189908266068\n","Epoch [58/100], Train Loss: 0.07666541635990143, Test Loss: 0.07406974583864212\n","Epoch [59/100], Train Loss: 0.0760783925652504, Test Loss: 0.07236889004707336\n","Epoch [60/100], Train Loss: 0.0743168368935585, Test Loss: 0.069980189204216\n","Epoch [61/100], Train Loss: 0.07147339731454849, Test Loss: 0.06922473758459091\n","Epoch [62/100], Train Loss: 0.0694785863161087, Test Loss: 0.0678839385509491\n","Epoch [63/100], Train Loss: 0.0700741782784462, Test Loss: 0.06875761598348618\n","Epoch [64/100], Train Loss: 0.07030172646045685, Test Loss: 0.06904938817024231\n","Epoch [65/100], Train Loss: 0.06953814625740051, Test Loss: 0.06803227216005325\n","Epoch [66/100], Train Loss: 0.06775720417499542, Test Loss: 0.067457415163517\n","Epoch [67/100], Train Loss: 0.0672142282128334, Test Loss: 0.06559792906045914\n","Epoch [68/100], Train Loss: 0.06379737704992294, Test Loss: 0.06489626318216324\n","Epoch [69/100], Train Loss: 0.0631566271185875, Test Loss: 0.06463290750980377\n","Epoch [70/100], Train Loss: 0.06327953189611435, Test Loss: 0.06467574834823608\n","Epoch [71/100], Train Loss: 0.06253042072057724, Test Loss: 0.06407474726438522\n","Epoch [72/100], Train Loss: 0.06261326372623444, Test Loss: 0.06302858889102936\n","Epoch [73/100], Train Loss: 0.06146436929702759, Test Loss: 0.062150802463293076\n","Epoch [74/100], Train Loss: 0.06028534471988678, Test Loss: 0.06114896014332771\n","Epoch [75/100], Train Loss: 0.05957069247961044, Test Loss: 0.06102370470762253\n","Epoch [76/100], Train Loss: 0.059487324208021164, Test Loss: 0.05966736376285553\n","Epoch [77/100], Train Loss: 0.057409245520830154, Test Loss: 0.05755683034658432\n","Epoch [78/100], Train Loss: 0.05502570420503616, Test Loss: 0.057785939425230026\n","Epoch [79/100], Train Loss: 0.053841810673475266, Test Loss: 0.05693625658750534\n","Epoch [80/100], Train Loss: 0.05401458218693733, Test Loss: 0.05666719377040863\n","Epoch [81/100], Train Loss: 0.05182259902358055, Test Loss: 0.0562560074031353\n","Epoch [82/100], Train Loss: 0.05071130022406578, Test Loss: 0.055666860193014145\n","Epoch [83/100], Train Loss: 0.050732675939798355, Test Loss: 0.05642044171690941\n","Epoch [84/100], Train Loss: 0.051640987396240234, Test Loss: 0.05560116842389107\n","Epoch [85/100], Train Loss: 0.05007270351052284, Test Loss: 0.05423940718173981\n","Epoch [86/100], Train Loss: 0.04957916587591171, Test Loss: 0.05272708460688591\n","Epoch [87/100], Train Loss: 0.04827847704291344, Test Loss: 0.05270626023411751\n","Epoch [88/100], Train Loss: 0.04730570316314697, Test Loss: 0.05263969674706459\n","Epoch [89/100], Train Loss: 0.0463353767991066, Test Loss: 0.05177963897585869\n","Epoch [90/100], Train Loss: 0.04581435024738312, Test Loss: 0.05130138248205185\n","Epoch [91/100], Train Loss: 0.04536857828497887, Test Loss: 0.0511527918279171\n","Epoch [92/100], Train Loss: 0.04607458785176277, Test Loss: 0.05095333978533745\n","Epoch [93/100], Train Loss: 0.04418989643454552, Test Loss: 0.05099042132496834\n","Epoch [94/100], Train Loss: 0.04445884749293327, Test Loss: 0.05027777701616287\n","Epoch [95/100], Train Loss: 0.04356623440980911, Test Loss: 0.049406569451093674\n","Epoch [96/100], Train Loss: 0.0421878881752491, Test Loss: 0.04875217750668526\n","Epoch [97/100], Train Loss: 0.04244924336671829, Test Loss: 0.047966357320547104\n","Epoch [98/100], Train Loss: 0.04224330186843872, Test Loss: 0.04747466370463371\n","Epoch [99/100], Train Loss: 0.04149677976965904, Test Loss: 0.047186706215143204\n","Epoch [100/100], Train Loss: 0.0412406288087368, Test Loss: 0.04628020152449608\n","Epoch [1/100], Train Loss: 0.7925562262535095, Test Loss: 0.6686263084411621\n","Epoch [2/100], Train Loss: 0.6857519149780273, Test Loss: 0.578012228012085\n","Epoch [3/100], Train Loss: 0.5937128663063049, Test Loss: 0.5043402314186096\n","Epoch [4/100], Train Loss: 0.522089421749115, Test Loss: 0.4457660913467407\n","Epoch [5/100], Train Loss: 0.46498075127601624, Test Loss: 0.3989603519439697\n","Epoch [6/100], Train Loss: 0.4166068434715271, Test Loss: 0.35817551612854004\n","Epoch [7/100], Train Loss: 0.37632420659065247, Test Loss: 0.3244704604148865\n","Epoch [8/100], Train Loss: 0.343467116355896, Test Loss: 0.29492101073265076\n","Epoch [9/100], Train Loss: 0.31611600518226624, Test Loss: 0.2707795798778534\n","Epoch [10/100], Train Loss: 0.291716992855072, Test Loss: 0.2503680884838104\n","Epoch [11/100], Train Loss: 0.27141910791397095, Test Loss: 0.23314201831817627\n","Epoch [12/100], Train Loss: 0.25256285071372986, Test Loss: 0.2180732935667038\n","Epoch [13/100], Train Loss: 0.23869873583316803, Test Loss: 0.20518028736114502\n","Epoch [14/100], Train Loss: 0.22478915750980377, Test Loss: 0.19404523074626923\n","Epoch [15/100], Train Loss: 0.21425336599349976, Test Loss: 0.1836680918931961\n","Epoch [16/100], Train Loss: 0.2036469280719757, Test Loss: 0.17442408204078674\n","Epoch [17/100], Train Loss: 0.19470731914043427, Test Loss: 0.16692885756492615\n","Epoch [18/100], Train Loss: 0.1856878697872162, Test Loss: 0.15936459600925446\n","Epoch [19/100], Train Loss: 0.1785084754228592, Test Loss: 0.15283556282520294\n","Epoch [20/100], Train Loss: 0.17148132622241974, Test Loss: 0.14675605297088623\n","Epoch [21/100], Train Loss: 0.1630762368440628, Test Loss: 0.14181774854660034\n","Epoch [22/100], Train Loss: 0.15682467818260193, Test Loss: 0.1377449482679367\n","Epoch [23/100], Train Loss: 0.1516849249601364, Test Loss: 0.1338489055633545\n","Epoch [24/100], Train Loss: 0.1468917280435562, Test Loss: 0.13022688031196594\n","Epoch [25/100], Train Loss: 0.14336353540420532, Test Loss: 0.1272515505552292\n","Epoch [26/100], Train Loss: 0.14046329259872437, Test Loss: 0.12622542679309845\n","Epoch [27/100], Train Loss: 0.13987842202186584, Test Loss: 0.12533673644065857\n","Epoch [28/100], Train Loss: 0.14033034443855286, Test Loss: 0.1229642704129219\n","Epoch [29/100], Train Loss: 0.13634710013866425, Test Loss: 0.12052764743566513\n","Epoch [30/100], Train Loss: 0.13487394154071808, Test Loss: 0.1185963973402977\n","Epoch [31/100], Train Loss: 0.13124677538871765, Test Loss: 0.11648991703987122\n","Epoch [32/100], Train Loss: 0.12841950356960297, Test Loss: 0.11443615704774857\n","Epoch [33/100], Train Loss: 0.12612532079219818, Test Loss: 0.11251062154769897\n","Epoch [34/100], Train Loss: 0.1235533058643341, Test Loss: 0.11134312301874161\n","Epoch [35/100], Train Loss: 0.12076811492443085, Test Loss: 0.1098674088716507\n","Epoch [36/100], Train Loss: 0.1192728653550148, Test Loss: 0.10869687795639038\n","Epoch [37/100], Train Loss: 0.11795620620250702, Test Loss: 0.1069788858294487\n","Epoch [38/100], Train Loss: 0.11653551459312439, Test Loss: 0.10668445378541946\n","Epoch [39/100], Train Loss: 0.1152253970503807, Test Loss: 0.10419297218322754\n","Epoch [40/100], Train Loss: 0.11346452683210373, Test Loss: 0.10232854634523392\n","Epoch [41/100], Train Loss: 0.10954269766807556, Test Loss: 0.10148371011018753\n","Epoch [42/100], Train Loss: 0.10823925584554672, Test Loss: 0.09978143125772476\n","Epoch [43/100], Train Loss: 0.1069314256310463, Test Loss: 0.10035111010074615\n","Epoch [44/100], Train Loss: 0.10779029875993729, Test Loss: 0.09970258176326752\n","Epoch [45/100], Train Loss: 0.10767778009176254, Test Loss: 0.09858134388923645\n","Epoch [46/100], Train Loss: 0.10482411831617355, Test Loss: 0.09714435786008835\n","Epoch [47/100], Train Loss: 0.10466567426919937, Test Loss: 0.09607967734336853\n","Epoch [48/100], Train Loss: 0.10297032445669174, Test Loss: 0.09508896619081497\n","Epoch [49/100], Train Loss: 0.10144400596618652, Test Loss: 0.09425674378871918\n","Epoch [50/100], Train Loss: 0.1011136993765831, Test Loss: 0.0933762714266777\n","Epoch [51/100], Train Loss: 0.1003895029425621, Test Loss: 0.0946388989686966\n","Epoch [52/100], Train Loss: 0.10175498574972153, Test Loss: 0.09606588631868362\n","Epoch [53/100], Train Loss: 0.10216204822063446, Test Loss: 0.09072776883840561\n","Epoch [54/100], Train Loss: 0.09613136947154999, Test Loss: 0.08974387496709824\n","Epoch [55/100], Train Loss: 0.0952497124671936, Test Loss: 0.08890099823474884\n","Epoch [56/100], Train Loss: 0.0939168855547905, Test Loss: 0.08834345638751984\n","Epoch [57/100], Train Loss: 0.09405122697353363, Test Loss: 0.0875370129942894\n","Epoch [58/100], Train Loss: 0.09234395623207092, Test Loss: 0.08663365244865417\n","Epoch [59/100], Train Loss: 0.09314141422510147, Test Loss: 0.08575137704610825\n","Epoch [60/100], Train Loss: 0.09176832437515259, Test Loss: 0.08514801412820816\n","Epoch [61/100], Train Loss: 0.09030870348215103, Test Loss: 0.08451715111732483\n","Epoch [62/100], Train Loss: 0.0889202430844307, Test Loss: 0.08389769494533539\n","Epoch [63/100], Train Loss: 0.08812399208545685, Test Loss: 0.08330555260181427\n","Epoch [64/100], Train Loss: 0.08928801864385605, Test Loss: 0.08274205029010773\n","Epoch [65/100], Train Loss: 0.08684589713811874, Test Loss: 0.08208770304918289\n","Epoch [66/100], Train Loss: 0.08626949787139893, Test Loss: 0.08154969662427902\n","Epoch [67/100], Train Loss: 0.08503971993923187, Test Loss: 0.08105606585741043\n","Epoch [68/100], Train Loss: 0.08463966101408005, Test Loss: 0.08055580407381058\n","Epoch [69/100], Train Loss: 0.08466888964176178, Test Loss: 0.08012156188488007\n","Epoch [70/100], Train Loss: 0.08364654332399368, Test Loss: 0.07981982827186584\n","Epoch [71/100], Train Loss: 0.08260183781385422, Test Loss: 0.07954397052526474\n","Epoch [72/100], Train Loss: 0.08347039669752121, Test Loss: 0.07914522290229797\n","Epoch [73/100], Train Loss: 0.08195816725492477, Test Loss: 0.07875048369169235\n","Epoch [74/100], Train Loss: 0.08020474761724472, Test Loss: 0.07815725356340408\n","Epoch [75/100], Train Loss: 0.08091193437576294, Test Loss: 0.07746696472167969\n","Epoch [76/100], Train Loss: 0.07964067161083221, Test Loss: 0.07679816335439682\n","Epoch [77/100], Train Loss: 0.07884519547224045, Test Loss: 0.07617545872926712\n","Epoch [78/100], Train Loss: 0.0786200687289238, Test Loss: 0.07546623051166534\n","Epoch [79/100], Train Loss: 0.07843879610300064, Test Loss: 0.0747242346405983\n","Epoch [80/100], Train Loss: 0.07686834782361984, Test Loss: 0.07414449751377106\n","Epoch [81/100], Train Loss: 0.0764814242720604, Test Loss: 0.07390931993722916\n","Epoch [82/100], Train Loss: 0.07656893134117126, Test Loss: 0.07326888293027878\n","Epoch [83/100], Train Loss: 0.07473721355199814, Test Loss: 0.07266152650117874\n","Epoch [84/100], Train Loss: 0.07474877685308456, Test Loss: 0.07178550958633423\n","Epoch [85/100], Train Loss: 0.07321090996265411, Test Loss: 0.07114182412624359\n","Epoch [86/100], Train Loss: 0.07323136925697327, Test Loss: 0.07044453918933868\n","Epoch [87/100], Train Loss: 0.07256857305765152, Test Loss: 0.06980576366186142\n","Epoch [88/100], Train Loss: 0.07180621474981308, Test Loss: 0.0693952664732933\n","Epoch [89/100], Train Loss: 0.07176689803600311, Test Loss: 0.06865324079990387\n","Epoch [90/100], Train Loss: 0.07036681473255157, Test Loss: 0.06794247031211853\n","Epoch [91/100], Train Loss: 0.06990996748209, Test Loss: 0.06729746609926224\n","Epoch [92/100], Train Loss: 0.06908305734395981, Test Loss: 0.06678133457899094\n","Epoch [93/100], Train Loss: 0.06880714744329453, Test Loss: 0.06638900935649872\n","Epoch [94/100], Train Loss: 0.06742040067911148, Test Loss: 0.06565801054239273\n","Epoch [95/100], Train Loss: 0.06748498976230621, Test Loss: 0.06493768095970154\n","Epoch [96/100], Train Loss: 0.06588201969861984, Test Loss: 0.06676556169986725\n","Epoch [97/100], Train Loss: 0.06787671148777008, Test Loss: 0.0660535916686058\n","Epoch [98/100], Train Loss: 0.0676182210445404, Test Loss: 0.06560495495796204\n","Epoch [99/100], Train Loss: 0.06694820523262024, Test Loss: 0.06505570560693741\n","Epoch [100/100], Train Loss: 0.06477339565753937, Test Loss: 0.06442344188690186\n","Epoch [1/100], Train Loss: 0.7340806126594543, Test Loss: 0.6187990307807922\n","Epoch [2/100], Train Loss: 0.634324312210083, Test Loss: 0.5366622805595398\n","Epoch [3/100], Train Loss: 0.5506837368011475, Test Loss: 0.4691791832447052\n","Epoch [4/100], Train Loss: 0.48706871271133423, Test Loss: 0.4149300456047058\n","Epoch [5/100], Train Loss: 0.4321584701538086, Test Loss: 0.37044546008110046\n","Epoch [6/100], Train Loss: 0.3917284309864044, Test Loss: 0.33302587270736694\n","Epoch [7/100], Train Loss: 0.35293108224868774, Test Loss: 0.3014425039291382\n","Epoch [8/100], Train Loss: 0.3216019570827484, Test Loss: 0.2751193642616272\n","Epoch [9/100], Train Loss: 0.29571911692619324, Test Loss: 0.2534556984901428\n","Epoch [10/100], Train Loss: 0.2754456698894501, Test Loss: 0.2353237271308899\n","Epoch [11/100], Train Loss: 0.25565534830093384, Test Loss: 0.2203158438205719\n","Epoch [12/100], Train Loss: 0.2415771782398224, Test Loss: 0.2083171159029007\n","Epoch [13/100], Train Loss: 0.22758325934410095, Test Loss: 0.19755816459655762\n","Epoch [14/100], Train Loss: 0.2177307903766632, Test Loss: 0.18802662193775177\n","Epoch [15/100], Train Loss: 0.20788727700710297, Test Loss: 0.17976373434066772\n","Epoch [16/100], Train Loss: 0.19847765564918518, Test Loss: 0.17239217460155487\n","Epoch [17/100], Train Loss: 0.1888124793767929, Test Loss: 0.16570138931274414\n","Epoch [18/100], Train Loss: 0.18298794329166412, Test Loss: 0.16061213612556458\n","Epoch [19/100], Train Loss: 0.1761673092842102, Test Loss: 0.154776930809021\n","Epoch [20/100], Train Loss: 0.16947388648986816, Test Loss: 0.14983375370502472\n","Epoch [21/100], Train Loss: 0.16268932819366455, Test Loss: 0.14709852635860443\n","Epoch [22/100], Train Loss: 0.15966910123825073, Test Loss: 0.14367224276065826\n","Epoch [23/100], Train Loss: 0.15628460049629211, Test Loss: 0.14033058285713196\n","Epoch [24/100], Train Loss: 0.15205010771751404, Test Loss: 0.13740509748458862\n","Epoch [25/100], Train Loss: 0.14752502739429474, Test Loss: 0.13607658445835114\n","Epoch [26/100], Train Loss: 0.14628304541110992, Test Loss: 0.13269300758838654\n","Epoch [27/100], Train Loss: 0.14211136102676392, Test Loss: 0.12972491979599\n","Epoch [28/100], Train Loss: 0.13767708837985992, Test Loss: 0.12686996161937714\n","Epoch [29/100], Train Loss: 0.1352066844701767, Test Loss: 0.12426988035440445\n","Epoch [30/100], Train Loss: 0.13238933682441711, Test Loss: 0.12229962646961212\n","Epoch [31/100], Train Loss: 0.12995566427707672, Test Loss: 0.11978646367788315\n","Epoch [32/100], Train Loss: 0.12446184456348419, Test Loss: 0.11775065213441849\n","Epoch [33/100], Train Loss: 0.12155646830797195, Test Loss: 0.11527328938245773\n","Epoch [34/100], Train Loss: 0.11828774213790894, Test Loss: 0.11336247622966766\n","Epoch [35/100], Train Loss: 0.11722949892282486, Test Loss: 0.11073850095272064\n","Epoch [36/100], Train Loss: 0.1144758090376854, Test Loss: 0.10876858234405518\n","Epoch [37/100], Train Loss: 0.11052753031253815, Test Loss: 0.10686704516410828\n","Epoch [38/100], Train Loss: 0.10775624960660934, Test Loss: 0.1047506332397461\n","Epoch [39/100], Train Loss: 0.10597191751003265, Test Loss: 0.10253746062517166\n","Epoch [40/100], Train Loss: 0.10366419702768326, Test Loss: 0.10320808738470078\n","Epoch [41/100], Train Loss: 0.10401707142591476, Test Loss: 0.103445865213871\n","Epoch [42/100], Train Loss: 0.10486044734716415, Test Loss: 0.09970632940530777\n","Epoch [43/100], Train Loss: 0.10003948956727982, Test Loss: 0.09731527417898178\n","Epoch [44/100], Train Loss: 0.09618301689624786, Test Loss: 0.0985584631562233\n","Epoch [45/100], Train Loss: 0.0993887260556221, Test Loss: 0.0966077670454979\n","Epoch [46/100], Train Loss: 0.09618964046239853, Test Loss: 0.0945693850517273\n","Epoch [47/100], Train Loss: 0.09424565732479095, Test Loss: 0.09347911179065704\n","Epoch [48/100], Train Loss: 0.09385105967521667, Test Loss: 0.09147372841835022\n","Epoch [49/100], Train Loss: 0.09053178876638412, Test Loss: 0.09013960510492325\n","Epoch [50/100], Train Loss: 0.08947422355413437, Test Loss: 0.08873213082551956\n","Epoch [51/100], Train Loss: 0.08870022743940353, Test Loss: 0.08719177544116974\n","Epoch [52/100], Train Loss: 0.08634984493255615, Test Loss: 0.08568880707025528\n","Epoch [53/100], Train Loss: 0.08461488038301468, Test Loss: 0.08609017729759216\n","Epoch [54/100], Train Loss: 0.08545128256082535, Test Loss: 0.08730614185333252\n","Epoch [55/100], Train Loss: 0.08578339964151382, Test Loss: 0.08559758216142654\n","Epoch [56/100], Train Loss: 0.08373402059078217, Test Loss: 0.08634421229362488\n","Epoch [57/100], Train Loss: 0.08593237400054932, Test Loss: 0.0873808041214943\n","Epoch [58/100], Train Loss: 0.08657456189393997, Test Loss: 0.08524022251367569\n","Epoch [59/100], Train Loss: 0.08441571891307831, Test Loss: 0.0838879719376564\n","Epoch [60/100], Train Loss: 0.08345994353294373, Test Loss: 0.08253586292266846\n","Epoch [61/100], Train Loss: 0.08232859522104263, Test Loss: 0.08136065304279327\n","Epoch [62/100], Train Loss: 0.08037421852350235, Test Loss: 0.0784822478890419\n","Epoch [63/100], Train Loss: 0.07704219967126846, Test Loss: 0.07748827338218689\n","Epoch [64/100], Train Loss: 0.074477419257164, Test Loss: 0.07631191611289978\n","Epoch [65/100], Train Loss: 0.07256026566028595, Test Loss: 0.0751846432685852\n","Epoch [66/100], Train Loss: 0.07229393720626831, Test Loss: 0.07386526465415955\n","Epoch [67/100], Train Loss: 0.06955144554376602, Test Loss: 0.07266245037317276\n","Epoch [68/100], Train Loss: 0.06926064938306808, Test Loss: 0.07114311307668686\n","Epoch [69/100], Train Loss: 0.06893743574619293, Test Loss: 0.0700242668390274\n","Epoch [70/100], Train Loss: 0.06694696843624115, Test Loss: 0.06920831650495529\n","Epoch [71/100], Train Loss: 0.06513432413339615, Test Loss: 0.06734030693769455\n","Epoch [72/100], Train Loss: 0.06367090344429016, Test Loss: 0.06750594824552536\n","Epoch [73/100], Train Loss: 0.0636933371424675, Test Loss: 0.06571167707443237\n","Epoch [74/100], Train Loss: 0.06145859137177467, Test Loss: 0.06491100043058395\n","Epoch [75/100], Train Loss: 0.05796993523836136, Test Loss: 0.06365978717803955\n","Epoch [76/100], Train Loss: 0.0595589205622673, Test Loss: 0.06283486634492874\n","Epoch [77/100], Train Loss: 0.05804042890667915, Test Loss: 0.0623326301574707\n","Epoch [78/100], Train Loss: 0.057827845215797424, Test Loss: 0.061397992074489594\n","Epoch [79/100], Train Loss: 0.05617769807577133, Test Loss: 0.06104697659611702\n","Epoch [80/100], Train Loss: 0.05661074072122574, Test Loss: 0.06058034300804138\n","Epoch [81/100], Train Loss: 0.05495748668909073, Test Loss: 0.05998297408223152\n","Epoch [82/100], Train Loss: 0.055209264159202576, Test Loss: 0.059555910527706146\n","Epoch [83/100], Train Loss: 0.05529859662055969, Test Loss: 0.05787786841392517\n","Epoch [84/100], Train Loss: 0.05297607555985451, Test Loss: 0.05710046738386154\n","Epoch [85/100], Train Loss: 0.051670271903276443, Test Loss: 0.056139182299375534\n","Epoch [86/100], Train Loss: 0.050323814153671265, Test Loss: 0.05572720989584923\n","Epoch [87/100], Train Loss: 0.04997517541050911, Test Loss: 0.055335190147161484\n","Epoch [88/100], Train Loss: 0.05002940818667412, Test Loss: 0.053712185472249985\n","Epoch [89/100], Train Loss: 0.04863479733467102, Test Loss: 0.05310772731900215\n","Epoch [90/100], Train Loss: 0.04816544055938721, Test Loss: 0.05329952389001846\n","Epoch [91/100], Train Loss: 0.04780757427215576, Test Loss: 0.054435934871435165\n","Epoch [92/100], Train Loss: 0.048950061202049255, Test Loss: 0.05233367532491684\n","Epoch [93/100], Train Loss: 0.04674847051501274, Test Loss: 0.051609426736831665\n","Epoch [94/100], Train Loss: 0.046260662376880646, Test Loss: 0.05074401944875717\n","Epoch [95/100], Train Loss: 0.04556663706898689, Test Loss: 0.0508783794939518\n","Epoch [96/100], Train Loss: 0.04560038074851036, Test Loss: 0.05029769986867905\n","Epoch [97/100], Train Loss: 0.04360640048980713, Test Loss: 0.04900278151035309\n","Epoch [98/100], Train Loss: 0.04195886105298996, Test Loss: 0.04948951303958893\n","Epoch [99/100], Train Loss: 0.04353928938508034, Test Loss: 0.049390505999326706\n","Epoch [100/100], Train Loss: 0.043781496584415436, Test Loss: 0.04902665317058563\n","Epoch [1/100], Train Loss: 0.7578167915344238, Test Loss: 0.6852267980575562\n","Epoch [2/100], Train Loss: 0.6877949833869934, Test Loss: 0.6264908909797668\n","Epoch [3/100], Train Loss: 0.6310884952545166, Test Loss: 0.5747783780097961\n","Epoch [4/100], Train Loss: 0.5802435874938965, Test Loss: 0.5272612571716309\n","Epoch [5/100], Train Loss: 0.533612072467804, Test Loss: 0.4831567108631134\n","Epoch [6/100], Train Loss: 0.49030375480651855, Test Loss: 0.44301387667655945\n","Epoch [7/100], Train Loss: 0.45232656598091125, Test Loss: 0.4060807526111603\n","Epoch [8/100], Train Loss: 0.4175662398338318, Test Loss: 0.3734813630580902\n","Epoch [9/100], Train Loss: 0.3850235641002655, Test Loss: 0.3450819253921509\n","Epoch [10/100], Train Loss: 0.357726514339447, Test Loss: 0.3200819194316864\n","Epoch [11/100], Train Loss: 0.3368522524833679, Test Loss: 0.2979549169540405\n","Epoch [12/100], Train Loss: 0.3154538571834564, Test Loss: 0.27877113223075867\n","Epoch [13/100], Train Loss: 0.2959654629230499, Test Loss: 0.2628006041049957\n","Epoch [14/100], Train Loss: 0.2793951630592346, Test Loss: 0.24879001080989838\n","Epoch [15/100], Train Loss: 0.26461249589920044, Test Loss: 0.23587411642074585\n","Epoch [16/100], Train Loss: 0.2536785900592804, Test Loss: 0.22478850185871124\n","Epoch [17/100], Train Loss: 0.24204891920089722, Test Loss: 0.21474440395832062\n","Epoch [18/100], Train Loss: 0.2300502508878708, Test Loss: 0.2051524966955185\n","Epoch [19/100], Train Loss: 0.2229398638010025, Test Loss: 0.19623513519763947\n","Epoch [20/100], Train Loss: 0.21224091947078705, Test Loss: 0.18797466158866882\n","Epoch [21/100], Train Loss: 0.20183812081813812, Test Loss: 0.17999786138534546\n","Epoch [22/100], Train Loss: 0.19493913650512695, Test Loss: 0.1723911613225937\n","Epoch [23/100], Train Loss: 0.1857745349407196, Test Loss: 0.16669097542762756\n","Epoch [24/100], Train Loss: 0.17927977442741394, Test Loss: 0.1607024371623993\n","Epoch [25/100], Train Loss: 0.17490379512310028, Test Loss: 0.15496547520160675\n","Epoch [26/100], Train Loss: 0.16758443415164948, Test Loss: 0.1499168574810028\n","Epoch [27/100], Train Loss: 0.16192388534545898, Test Loss: 0.14515306055545807\n","Epoch [28/100], Train Loss: 0.15619109570980072, Test Loss: 0.14235703647136688\n","Epoch [29/100], Train Loss: 0.1548020839691162, Test Loss: 0.13816161453723907\n","Epoch [30/100], Train Loss: 0.15108337998390198, Test Loss: 0.13581116497516632\n","Epoch [31/100], Train Loss: 0.14659585058689117, Test Loss: 0.1334802359342575\n","Epoch [32/100], Train Loss: 0.14446920156478882, Test Loss: 0.13098320364952087\n","Epoch [33/100], Train Loss: 0.14139269292354584, Test Loss: 0.12754711508750916\n","Epoch [34/100], Train Loss: 0.137549489736557, Test Loss: 0.12463332712650299\n","Epoch [35/100], Train Loss: 0.13243310153484344, Test Loss: 0.12185301631689072\n","Epoch [36/100], Train Loss: 0.12980321049690247, Test Loss: 0.11942034214735031\n","Epoch [37/100], Train Loss: 0.1263550966978073, Test Loss: 0.11751433461904526\n","Epoch [38/100], Train Loss: 0.12465736269950867, Test Loss: 0.1148892492055893\n","Epoch [39/100], Train Loss: 0.1209021583199501, Test Loss: 0.11517062783241272\n","Epoch [40/100], Train Loss: 0.12091591954231262, Test Loss: 0.11162862181663513\n","Epoch [41/100], Train Loss: 0.11657753586769104, Test Loss: 0.10906446725130081\n","Epoch [42/100], Train Loss: 0.11532832682132721, Test Loss: 0.10730253159999847\n","Epoch [43/100], Train Loss: 0.11190245300531387, Test Loss: 0.10525287687778473\n","Epoch [44/100], Train Loss: 0.10844580084085464, Test Loss: 0.10380163788795471\n","Epoch [45/100], Train Loss: 0.10669473558664322, Test Loss: 0.10351326316595078\n","Epoch [46/100], Train Loss: 0.10656751692295074, Test Loss: 0.10140839219093323\n","Epoch [47/100], Train Loss: 0.1041286438703537, Test Loss: 0.09916342794895172\n","Epoch [48/100], Train Loss: 0.10215212404727936, Test Loss: 0.09762191027402878\n","Epoch [49/100], Train Loss: 0.10150450468063354, Test Loss: 0.09513965994119644\n","Epoch [50/100], Train Loss: 0.09746503084897995, Test Loss: 0.09220409393310547\n","Epoch [51/100], Train Loss: 0.09482436627149582, Test Loss: 0.09083612263202667\n","Epoch [52/100], Train Loss: 0.0925767794251442, Test Loss: 0.0893721655011177\n","Epoch [53/100], Train Loss: 0.0919213593006134, Test Loss: 0.08773539960384369\n","Epoch [54/100], Train Loss: 0.09097708016633987, Test Loss: 0.086326003074646\n","Epoch [55/100], Train Loss: 0.08844997733831406, Test Loss: 0.08497188985347748\n","Epoch [56/100], Train Loss: 0.08646819740533829, Test Loss: 0.08370348066091537\n","Epoch [57/100], Train Loss: 0.08520717173814774, Test Loss: 0.08281311392784119\n","Epoch [58/100], Train Loss: 0.08615779131650925, Test Loss: 0.08186579495668411\n","Epoch [59/100], Train Loss: 0.0846058577299118, Test Loss: 0.08006143569946289\n","Epoch [60/100], Train Loss: 0.08264341950416565, Test Loss: 0.07828065752983093\n","Epoch [61/100], Train Loss: 0.08032103627920151, Test Loss: 0.07824940234422684\n","Epoch [62/100], Train Loss: 0.08129991590976715, Test Loss: 0.07683117687702179\n","Epoch [63/100], Train Loss: 0.07896880060434341, Test Loss: 0.07533533871173859\n","Epoch [64/100], Train Loss: 0.0768938660621643, Test Loss: 0.07587820291519165\n","Epoch [65/100], Train Loss: 0.07704142481088638, Test Loss: 0.07504177838563919\n","Epoch [66/100], Train Loss: 0.07650250196456909, Test Loss: 0.07363582402467728\n","Epoch [67/100], Train Loss: 0.07512234896421432, Test Loss: 0.07351934909820557\n","Epoch [68/100], Train Loss: 0.07471071928739548, Test Loss: 0.07250650972127914\n","Epoch [69/100], Train Loss: 0.07364127039909363, Test Loss: 0.07159658521413803\n","Epoch [70/100], Train Loss: 0.07248002290725708, Test Loss: 0.07123422622680664\n","Epoch [71/100], Train Loss: 0.07118851691484451, Test Loss: 0.07055643945932388\n","Epoch [72/100], Train Loss: 0.07018470764160156, Test Loss: 0.06964845955371857\n","Epoch [73/100], Train Loss: 0.0685034841299057, Test Loss: 0.0688527375459671\n","Epoch [74/100], Train Loss: 0.06746187806129456, Test Loss: 0.06801339983940125\n","Epoch [75/100], Train Loss: 0.06594935059547424, Test Loss: 0.06698921322822571\n","Epoch [76/100], Train Loss: 0.06586381793022156, Test Loss: 0.06728548556566238\n","Epoch [77/100], Train Loss: 0.06711617857217789, Test Loss: 0.0659794732928276\n","Epoch [78/100], Train Loss: 0.06465139985084534, Test Loss: 0.06437887251377106\n","Epoch [79/100], Train Loss: 0.06428104639053345, Test Loss: 0.06672130525112152\n","Epoch [80/100], Train Loss: 0.06655828654766083, Test Loss: 0.06570208072662354\n","Epoch [81/100], Train Loss: 0.06545514613389969, Test Loss: 0.06283608078956604\n","Epoch [82/100], Train Loss: 0.0619635134935379, Test Loss: 0.06161940097808838\n","Epoch [83/100], Train Loss: 0.060790084302425385, Test Loss: 0.06041685864329338\n","Epoch [84/100], Train Loss: 0.059278473258018494, Test Loss: 0.0605224147439003\n","Epoch [85/100], Train Loss: 0.059456001967191696, Test Loss: 0.05969877913594246\n","Epoch [86/100], Train Loss: 0.05906769633293152, Test Loss: 0.05873360112309456\n","Epoch [87/100], Train Loss: 0.057369425892829895, Test Loss: 0.058323439210653305\n","Epoch [88/100], Train Loss: 0.05860835313796997, Test Loss: 0.05753098428249359\n","Epoch [89/100], Train Loss: 0.057058997452259064, Test Loss: 0.056824058294296265\n","Epoch [90/100], Train Loss: 0.054894790053367615, Test Loss: 0.055871106684207916\n","Epoch [91/100], Train Loss: 0.054175928235054016, Test Loss: 0.054756004363298416\n","Epoch [92/100], Train Loss: 0.05357220396399498, Test Loss: 0.05411997810006142\n","Epoch [93/100], Train Loss: 0.05205841362476349, Test Loss: 0.05366518348455429\n","Epoch [94/100], Train Loss: 0.052031975239515305, Test Loss: 0.05360942706465721\n","Epoch [95/100], Train Loss: 0.051312606781721115, Test Loss: 0.05305389687418938\n","Epoch [96/100], Train Loss: 0.05134871229529381, Test Loss: 0.052777376025915146\n","Epoch [97/100], Train Loss: 0.05109161138534546, Test Loss: 0.0523708276450634\n","Epoch [98/100], Train Loss: 0.051280274987220764, Test Loss: 0.05220216140151024\n","Epoch [99/100], Train Loss: 0.04980925843119621, Test Loss: 0.05168979614973068\n","Epoch [100/100], Train Loss: 0.048894237726926804, Test Loss: 0.05102784186601639\n","Epoch [1/100], Train Loss: 0.6687865257263184, Test Loss: 0.5651659965515137\n","Epoch [2/100], Train Loss: 0.5840267539024353, Test Loss: 0.5022851228713989\n","Epoch [3/100], Train Loss: 0.5200024843215942, Test Loss: 0.44669434428215027\n","Epoch [4/100], Train Loss: 0.4658488929271698, Test Loss: 0.3973170518875122\n","Epoch [5/100], Train Loss: 0.4168113172054291, Test Loss: 0.354836106300354\n","Epoch [6/100], Train Loss: 0.3755607008934021, Test Loss: 0.32069408893585205\n","Epoch [7/100], Train Loss: 0.3417084813117981, Test Loss: 0.29371002316474915\n","Epoch [8/100], Train Loss: 0.3161662817001343, Test Loss: 0.27075308561325073\n","Epoch [9/100], Train Loss: 0.293245792388916, Test Loss: 0.25128674507141113\n","Epoch [10/100], Train Loss: 0.27473610639572144, Test Loss: 0.2344285100698471\n","Epoch [11/100], Train Loss: 0.2583577036857605, Test Loss: 0.22022511065006256\n","Epoch [12/100], Train Loss: 0.24267950654029846, Test Loss: 0.20849306881427765\n","Epoch [13/100], Train Loss: 0.23118995130062103, Test Loss: 0.19866766035556793\n","Epoch [14/100], Train Loss: 0.2208368331193924, Test Loss: 0.18981458246707916\n","Epoch [15/100], Train Loss: 0.2111922949552536, Test Loss: 0.18140274286270142\n","Epoch [16/100], Train Loss: 0.2036321759223938, Test Loss: 0.1739637553691864\n","Epoch [17/100], Train Loss: 0.1953928917646408, Test Loss: 0.16743779182434082\n","Epoch [18/100], Train Loss: 0.1874813288450241, Test Loss: 0.16094185411930084\n","Epoch [19/100], Train Loss: 0.1801658421754837, Test Loss: 0.15480037033557892\n","Epoch [20/100], Train Loss: 0.17333467304706573, Test Loss: 0.1490418016910553\n","Epoch [21/100], Train Loss: 0.16566376388072968, Test Loss: 0.14396953582763672\n","Epoch [22/100], Train Loss: 0.15995138883590698, Test Loss: 0.13949763774871826\n","Epoch [23/100], Train Loss: 0.15454725921154022, Test Loss: 0.1353546530008316\n","Epoch [24/100], Train Loss: 0.14796099066734314, Test Loss: 0.1342078000307083\n","Epoch [25/100], Train Loss: 0.14820683002471924, Test Loss: 0.13100886344909668\n","Epoch [26/100], Train Loss: 0.14356793463230133, Test Loss: 0.12795858085155487\n","Epoch [27/100], Train Loss: 0.13943606615066528, Test Loss: 0.12511369585990906\n","Epoch [28/100], Train Loss: 0.13728772103786469, Test Loss: 0.1232159286737442\n","Epoch [29/100], Train Loss: 0.13167943060398102, Test Loss: 0.12193913012742996\n","Epoch [30/100], Train Loss: 0.13155540823936462, Test Loss: 0.12039215117692947\n","Epoch [31/100], Train Loss: 0.12878009676933289, Test Loss: 0.12001867592334747\n","Epoch [32/100], Train Loss: 0.1283651441335678, Test Loss: 0.11682260036468506\n","Epoch [33/100], Train Loss: 0.12191510200500488, Test Loss: 0.1156197115778923\n","Epoch [34/100], Train Loss: 0.12159831076860428, Test Loss: 0.11515668779611588\n","Epoch [35/100], Train Loss: 0.11853480339050293, Test Loss: 0.11120066791772842\n","Epoch [36/100], Train Loss: 0.11285065114498138, Test Loss: 0.10955345630645752\n","Epoch [37/100], Train Loss: 0.11270151287317276, Test Loss: 0.108034648001194\n","Epoch [38/100], Train Loss: 0.10964719951152802, Test Loss: 0.10635444521903992\n","Epoch [39/100], Train Loss: 0.11001289635896683, Test Loss: 0.10479116439819336\n","Epoch [40/100], Train Loss: 0.10779023915529251, Test Loss: 0.10296563059091568\n","Epoch [41/100], Train Loss: 0.106174536049366, Test Loss: 0.1009579449892044\n","Epoch [42/100], Train Loss: 0.10296954959630966, Test Loss: 0.09925419092178345\n","Epoch [43/100], Train Loss: 0.10062723606824875, Test Loss: 0.09754454344511032\n","Epoch [44/100], Train Loss: 0.09866378456354141, Test Loss: 0.09599032253026962\n","Epoch [45/100], Train Loss: 0.09787780046463013, Test Loss: 0.09442677348852158\n","Epoch [46/100], Train Loss: 0.09510838985443115, Test Loss: 0.09281526505947113\n","Epoch [47/100], Train Loss: 0.0952107384800911, Test Loss: 0.0907217264175415\n","Epoch [48/100], Train Loss: 0.09164634346961975, Test Loss: 0.09114173799753189\n","Epoch [49/100], Train Loss: 0.09241227805614471, Test Loss: 0.08817309886217117\n","Epoch [50/100], Train Loss: 0.09135941416025162, Test Loss: 0.08699751645326614\n","Epoch [51/100], Train Loss: 0.08969313651323318, Test Loss: 0.08605660498142242\n","Epoch [52/100], Train Loss: 0.0880347341299057, Test Loss: 0.08484544605016708\n","Epoch [53/100], Train Loss: 0.08720474690198898, Test Loss: 0.0832420140504837\n","Epoch [54/100], Train Loss: 0.08610605448484421, Test Loss: 0.08179188519716263\n","Epoch [55/100], Train Loss: 0.08502712845802307, Test Loss: 0.08086691796779633\n","Epoch [56/100], Train Loss: 0.08319833874702454, Test Loss: 0.0803317204117775\n","Epoch [57/100], Train Loss: 0.08267189562320709, Test Loss: 0.07912895083427429\n","Epoch [58/100], Train Loss: 0.08070635050535202, Test Loss: 0.07827974855899811\n","Epoch [59/100], Train Loss: 0.07989799976348877, Test Loss: 0.07746764272451401\n","Epoch [60/100], Train Loss: 0.07942137867212296, Test Loss: 0.0767052173614502\n","Epoch [61/100], Train Loss: 0.07843121886253357, Test Loss: 0.07823974639177322\n","Epoch [62/100], Train Loss: 0.07841289788484573, Test Loss: 0.07738857716321945\n","Epoch [63/100], Train Loss: 0.07918772101402283, Test Loss: 0.0765131488442421\n","Epoch [64/100], Train Loss: 0.07734686136245728, Test Loss: 0.07563319802284241\n","Epoch [65/100], Train Loss: 0.0769520252943039, Test Loss: 0.0745883360505104\n","Epoch [66/100], Train Loss: 0.0761130228638649, Test Loss: 0.07438217103481293\n","Epoch [67/100], Train Loss: 0.07498250901699066, Test Loss: 0.07301076501607895\n","Epoch [68/100], Train Loss: 0.0746999979019165, Test Loss: 0.07185522466897964\n","Epoch [69/100], Train Loss: 0.0730498805642128, Test Loss: 0.07081669569015503\n","Epoch [70/100], Train Loss: 0.07063614577054977, Test Loss: 0.06985088437795639\n","Epoch [71/100], Train Loss: 0.07063854485750198, Test Loss: 0.06909865140914917\n","Epoch [72/100], Train Loss: 0.07027994096279144, Test Loss: 0.06910958141088486\n","Epoch [73/100], Train Loss: 0.06842108070850372, Test Loss: 0.06895200908184052\n","Epoch [74/100], Train Loss: 0.06879325211048126, Test Loss: 0.06780010461807251\n","Epoch [75/100], Train Loss: 0.06823119521141052, Test Loss: 0.06692726910114288\n","Epoch [76/100], Train Loss: 0.06600365042686462, Test Loss: 0.06550318002700806\n","Epoch [77/100], Train Loss: 0.06626483052968979, Test Loss: 0.06498918682336807\n","Epoch [78/100], Train Loss: 0.0651337280869484, Test Loss: 0.06419073045253754\n","Epoch [79/100], Train Loss: 0.06382428109645844, Test Loss: 0.06360573321580887\n","Epoch [80/100], Train Loss: 0.06379583477973938, Test Loss: 0.06600043922662735\n","Epoch [81/100], Train Loss: 0.0662371888756752, Test Loss: 0.0626014918088913\n","Epoch [82/100], Train Loss: 0.061647143214941025, Test Loss: 0.06191731244325638\n","Epoch [83/100], Train Loss: 0.06093967705965042, Test Loss: 0.061573196202516556\n","Epoch [84/100], Train Loss: 0.06122150644659996, Test Loss: 0.06106828525662422\n","Epoch [85/100], Train Loss: 0.06149173155426979, Test Loss: 0.06082190200686455\n","Epoch [86/100], Train Loss: 0.060482919216156006, Test Loss: 0.061823416501283646\n","Epoch [87/100], Train Loss: 0.06078897416591644, Test Loss: 0.05789673328399658\n","Epoch [88/100], Train Loss: 0.05654488503932953, Test Loss: 0.057467930018901825\n","Epoch [89/100], Train Loss: 0.05629486218094826, Test Loss: 0.05697757378220558\n","Epoch [90/100], Train Loss: 0.05501435324549675, Test Loss: 0.056583430618047714\n","Epoch [91/100], Train Loss: 0.0553913377225399, Test Loss: 0.056215621531009674\n","Epoch [92/100], Train Loss: 0.05453771352767944, Test Loss: 0.055918388068675995\n","Epoch [93/100], Train Loss: 0.05441725254058838, Test Loss: 0.05480270832777023\n","Epoch [94/100], Train Loss: 0.05280500277876854, Test Loss: 0.0541122741997242\n","Epoch [95/100], Train Loss: 0.05181313678622246, Test Loss: 0.05361754819750786\n","Epoch [96/100], Train Loss: 0.05175264552235603, Test Loss: 0.05314058065414429\n","Epoch [97/100], Train Loss: 0.051393914967775345, Test Loss: 0.05244194343686104\n","Epoch [98/100], Train Loss: 0.04996262863278389, Test Loss: 0.05189928039908409\n","Epoch [99/100], Train Loss: 0.05023549869656563, Test Loss: 0.051318731158971786\n","Epoch [100/100], Train Loss: 0.04856701195240021, Test Loss: 0.05084292218089104\n","\n","Performance Metrics:\n","ROC AUC: 0.9177 ± 0.0198\n","Precision: 0.9805\n","Recall: 0.9887\n","F1 Score: 0.9846\n","Accuracy: 0.9834\n","False Alarm Rate (FAR): 0.0226 ± 0.0088\n"]}]}]}